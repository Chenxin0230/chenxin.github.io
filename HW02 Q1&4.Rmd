---
title: "HW02_Chenxin"
output:
  pdf_document: default
  html_document: default
date: "2024-02-20"
---

```{r}
library(tidyverse)
library(ggplot2)
library(modelr)
library(rsample)
library(mosaic)
library(pROC) # For ROC curve analysis
library(class)
library(kknn)
library(foreach)
library(doParallel)
library(ModelMetrics)
library(gamlr) # for lasso-penalized logistic regression
library(caret) # For data splitting and preprocessing
```


# Q4 Mushroom classification
```{r}
# data processing
mush = read.csv('/Users/vita/Desktop/mushrooms.csv')
mush = na.omit(mush)
# Remove columns with only one unique value (including factors with one level)
mush = mush[sapply(mush, function(x) length(unique(x)) > 1)]
# Convert all categorical variables to factors
mush[] <- lapply(mush, factor)
head(mush,)
```

```{r}

# Convert factors to dummy variables
# caret's dummyVars function can be used for one-hot encoding
dummies <- dummyVars(" ~ .", data = mush)
mushrooms_transformed <- predict(dummies, newdata = mush)

# Convert to data frame
mushrooms_df <- data.frame(mushrooms_transformed)
# Separate features and target variable
y <- mushrooms_df[, "class.e"] # based on target variable
X <- mushrooms_df[, -1] # Exclude the target variable, selects all columns except the first one

# Check dimensions
dim(X)
length(y)
```

```{r}
# (1)  Model Training : Lasso-penalized logistic regression 
# Use lambda to train the final lasso-penalized logistic regression model on the entire training set
# (1)  Splitting data into training (80%) and test (20%) sets
# It's better to re-run cv.gamlr using just the training set
trainIndex <- createDataPartition(y, p = .8, list = FALSE)
X_train <- X[trainIndex, ]
y_train <- y[trainIndex]
X_test <- X[-trainIndex, ]
y_test <- y[-trainIndex]

model <- cv.gamlr(X_train, y_train, family="binomial")
model
```

```{r}
# Plot to visualize lambda selection (optional step for visualization)
plot(model)
best_lambda <- model$lambda.min # Extract the best lambda
best_lambda
```

```{r}
# Proceed with ROC curve analysis and further evaluation as previously described
# (2) Make predictions on the test set 
predictions <- predict(model, newdata=X_test, type="response")
# (3) Evaluate the Model with ROC Curve
# Generating ROC curve and calculating AUC
roc_result <- roc(y_test, predictions)
plot(roc_result, main="ROC Curve")

# Finding optimal threshold
coords(roc_result, "best", ret="threshold")

```


```{r}
# Using optimal threshold
optimal_threshold <- coords(roc_result, "best", ret="threshold")
optimal_threshold
```
````{r}
# (4) Calculate FPR and TPR at the Optimal Threshold
# Now apply the thresholding logic
optimal_threshold <- matrix(as.numeric(optimal_threshold), nrow = nrow(predictions), ncol = ncol(predictions), byrow = TRUE)

predictions_binary <- ifelse(predictions > optimal_threshold, 1, 0)
```


```{r}
# Create a confusion matrix
conf_matrix <- table(Predicted = predictions_binary, Actual = y_test)

conf_matrix

# Calculate TPR and FPR from the confusion matrix
# True Positive Rate (Sensitivity)
false_positive_rate <- conf_matrix[2, 1] / sum(conf_matrix[2, ])
true_positive_rate <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
print(paste("False Positive Rate:", false_positive_rate))
print(paste("True Positive Rate:", true_positive_rate))

```



# Write a short report on the best-performing model you can find using lasso-penalized logistic regression. Evaluate the out-of-sample performance of your model using a ROC curve. Based on this ROC curve, recommend a probability threshold for declaring a mushroom poisonous. 
# Answer: The ROC curve to be a perfect diagonal line, which suggests that the model performs no better than random guessing. It would not be appropriate to recommend a probability threshold, as the model does not discriminate between the classes better than chance.

# How well does your model perform at this threshold, as measured by false positive rate and true positive rate?



